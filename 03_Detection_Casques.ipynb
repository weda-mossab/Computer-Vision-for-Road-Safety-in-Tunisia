{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Business Understanding (Phase CRISP-DM)\n",
        "### Contexte Global :\n",
        "En Tunisie, la sécurité routière est un enjeu de santé publique majeur. Selon l'Observatoire National de la Sécurité Routière (ONSR), le non-respect du code de la route et le défaut d'équipement de sécurité (comme le casque) sont les causes principales de mortalité. Les méthodes de surveillance manuelles étant limitées, l'automatisation par Intelligence Artificielle devient une nécessité.\n",
        "\n",
        "### Objectif du Projet Final :\n",
        "Développer une solution intelligente de surveillance routière capable de :\n",
        "\n",
        "##### * Identifier les véhicules (Détection de plaques).\n",
        "\n",
        "##### * Extraire les informations d'immatriculation (OCR des plaques tunisiennes).\n",
        "\n",
        "##### * Vérifier la conformité sécuritaire (Détection du port du casque).\n",
        "\n",
        "### Spécificité de ce module :\n",
        "\n",
        "#### 01_Detection_Plaques.ipynb :\n",
        "Ce notebook se concentre sur la localisation précise des plaques d'immatriculation sur les véhicules en circulation.\n",
        "\n",
        "#### 02_OCR_CNN_From_Scratch.ipynb :\n",
        "Ce notebook est dédié à la lecture optique des caractères (OCR) sur les plaques détectées, en utilisant une architecture CNN conçue \"from scratch\".\n",
        "\n",
        "#### 03_Detection_Casques.ipynb :\n",
        "Ce notebook traite de la détection du port du casque, permettant d'identifier les infractions de sécurité des motards.\n"
      ],
      "metadata": {
        "id": "-vl1VuSCRBtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WrR1tZdyUM14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE2lov2CP11n"
      },
      "outputs": [],
      "source": [
        "# Installation des librairies manquantes sur Colab\n",
        "!pip install -q ultralytics\n",
        "!pip install -q opencv-python-headless\n",
        "\n",
        "# Importations\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Télécharger le dataset spécifique aux casques\n",
        "path = kagglehub.dataset_download(\"andrewmvd/helmet-detection\")\n",
        "print(\"Dataset téléchargé dans :\", path)\n",
        "\n",
        "# 2. Explorer le contenu du dossier racine\n",
        "print(\"Contenu du dossier racine :\")\n",
        "print(os.listdir(path))\n",
        "\n",
        "# 3. Localiser les dossiers d'images et d'annotations\n",
        "# Dans ce dataset, la structure est : /images et /annotations\n",
        "image_dir = pathlib.Path(path) / \"images\"\n",
        "annot_dir = pathlib.Path(path) / \"annotations\"\n",
        "\n",
        "# 4. Vérification et exploration\n",
        "if image_dir.exists() and annot_dir.exists():\n",
        "    # Note : ce dataset utilise des fichiers .png\n",
        "    image_files = list(image_dir.glob(\"*.png\"))\n",
        "    annot_files = list(annot_dir.glob(\"*.xml\"))\n",
        "\n",
        "    print(f\"\\n--- Exploration réussie ---\")\n",
        "    print(f\"Nombre d'images trouvées (.png) : {len(image_files)}\")\n",
        "    print(f\"Nombre d'annotations trouvées (.xml) : {len(annot_files)}\")\n",
        "    print(\"Exemples d'images :\", [f.name for f in image_files[:3]])\n",
        "else:\n",
        "    print(\"\\n[ERREUR] Structure non conforme — exploration manuelle :\")\n",
        "    !ls {path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhzqYCOoV0MP",
        "outputId": "155b7fda-a7d5-49c0-d3bb-d05bbc82ec4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'helmet-detection' dataset.\n",
            "Dataset téléchargé dans : /kaggle/input/helmet-detection\n",
            "Contenu du dossier racine :\n",
            "['annotations', 'images']\n",
            "\n",
            "--- Exploration réussie ---\n",
            "Nombre d'images trouvées (.png) : 764\n",
            "Nombre d'annotations trouvées (.xml) : 764\n",
            "Exemples d'images : ['BikesHelmets719.png', 'BikesHelmets219.png', 'BikesHelmets18.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation (Extraction des images)"
      ],
      "metadata": {
        "id": "opubjQ07W1OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Relancement de l'extraction avec les labels : 'With Helmet' et 'Without Helmet'...\")\n",
        "\n",
        "# On nettoie pour repartir sur de bonnes bases\n",
        "import shutil\n",
        "shutil.rmtree(str(class_dir), ignore_errors=True)\n",
        "for label in ['with_helmet', 'without_helmet']:\n",
        "    (class_dir / label).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for xml_file in tqdm(annot_files):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    img_name_stem = pathlib.Path(xml_file).stem\n",
        "    img_path = image_dir / f\"{img_name_stem}.png\"\n",
        "\n",
        "    image = cv2.imread(str(img_path))\n",
        "    if image is None: continue\n",
        "\n",
        "    for i, obj in enumerate(root.findall('object')):\n",
        "        raw_label = obj.find('name').text # Ici on récupère 'With Helmet' ou 'Without Helmet'\n",
        "\n",
        "        # Mapping exact\n",
        "        if raw_label == 'With Helmet':\n",
        "            final_label = 'with_helmet'\n",
        "        elif raw_label == 'Without Helmet':\n",
        "            final_label = 'without_helmet'\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "        crop = image[ymin:ymax, xmin:xmax]\n",
        "        if crop.size > 0:\n",
        "            save_name = f\"{img_name_stem}_{i}.jpg\"\n",
        "            cv2.imwrite(str(class_dir / final_label / save_name), crop)\n",
        "\n",
        "print(f\"\\nExtraction réussie !\")\n",
        "print(f\"Images 'With Helmet' : {len(list((class_dir/'with_helmet').glob('*')))}\")\n",
        "print(f\"Images 'Without Helmet' : {len(list((class_dir/'without_helmet').glob('*')))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e939chg5WQ0H",
        "outputId": "1a9f9ca1-6a82-495f-cd13-25e2dec727e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relancement de l'extraction avec les labels : 'With Helmet' et 'Without Helmet'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 764/764 [00:13<00:00, 55.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extraction réussie !\n",
            "Images 'With Helmet' : 952\n",
            "Images 'Without Helmet' : 482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extraction/Cropping"
      ],
      "metadata": {
        "id": "Nipwx2g_YfXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration : Normalisation et Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,            # Transformation des pixels de [0-255] à [0-1]\n",
        "    rotation_range=15,         # Rotation aléatoire de 15 degrés\n",
        "    width_shift_range=0.1,     # Décalage horizontal\n",
        "    height_shift_range=0.1,    # Décalage vertical\n",
        "    horizontal_flip=True,      # Retournement horizontal (miroir)\n",
        "    validation_split=0.2       # 20% des données réservées pour le test (Validation)\n",
        ")\n",
        "\n",
        "# Générateur pour l'entraînement (80% des données)\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    class_dir,\n",
        "    target_size=(224, 224),    # Taille standard pour MobileNetV2\n",
        "    batch_size=32,\n",
        "    class_mode='binary',       # Classification binaire (Avec/Sans casque)\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Générateur pour la validation (20% des données)\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    class_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H28kEZiXeSX",
        "outputId": "8c6b5890-41fb-4140-a466-36dc620f19b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1148 images belonging to 2 classes.\n",
            "Found 286 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Division du Dataset (Code split-folders)"
      ],
      "metadata": {
        "id": "rfGG63UScmfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installer la lib (si pas déjà fait en haut)\n",
        "!pip install -q split-folders\n",
        "\n",
        "import splitfolders\n",
        "\n",
        "# 2. Définir le dossier source (celui où tu as extrait tes images)\n",
        "# class_dir était : /content/helmet_data\n",
        "input_folder = \"/content/helmet_data\"\n",
        "\n",
        "# 3. Définir le dossier de sortie pour la division\n",
        "output_folder = \"/content/helmet_split\"\n",
        "\n",
        "# 4. Exécuter la division (80% Train, 10% Val, 10% Test)\n",
        "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .1, .1))\n",
        "\n",
        "print(\"Division terminée ! Structure créée dans :\", output_folder)\n",
        "\n",
        "# Vérification\n",
        "!ls /content/helmet_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEz6FqHbcm37",
        "outputId": "e4df2445-b832-4188-f5eb-62fcca4b3991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 1434 files [00:00, 10521.90 files/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Division terminée ! Structure créée dans : /content/helmet_split\n",
            "test  train  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Création des Générateurs de Données"
      ],
      "metadata": {
        "id": "Xkv6FGqodcko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemins vers les nouveaux dossiers\n",
        "TRAIN_DIR = \"/content/helmet_split/train\"\n",
        "VAL_DIR = \"/content/helmet_split/val\"\n",
        "TEST_DIR = \"/content/helmet_split/test\"\n",
        "\n",
        "# 1. Augmentation pour le Train\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# 2. Simple normalisation pour Val et Test (pas de transformations)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 3. Création des générateurs\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False # Très important pour l'évaluation finale\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyJLMGAdddxR",
        "outputId": "b35bdb4b-ad52-4f0c-dcd3-dd18f67a6367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1146 images belonging to 2 classes.\n",
            "Found 143 images belonging to 2 classes.\n",
            "Found 145 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Modeling (Transfer Learning)"
      ],
      "metadata": {
        "id": "7cv5IHxfd7ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Charger la base MobileNetV2 pré-entraînée\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# 2. Geler la base pour conserver l'apprentissage initial\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Ajouter la tête de classification adaptée à notre problème\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),  # au lieu de flatten pour transformer l'image en un seule vecteur\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),              # Évite le surapprentissage (Overfitting)\n",
        "    layers.Dense(1, activation='sigmoid') # Sortie : 0 (Pas de casque) ou 1 (Avec casque)\n",
        "])\n",
        "\n",
        "# 4. Compilation avec l'optimiseur Adam\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WATChGUyd4wM",
        "outputId": "143e2b1a-8dff-49fc-9a38-abc3d064b5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training"
      ],
      "metadata": {
        "id": "x5x2SrDcfPMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancement de l'entraînement (Vérifie bien que tu as activé le GPU dans Colab !)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Évaluation sur le dataset de TEST (le juge final)\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"\\nPrécision finale sur le TEST : {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml_Ql-HEfRbq",
        "outputId": "4a2192e3-3503-4541-c147-f2ebbd830d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.6310 - loss: 0.7857 - val_accuracy: 0.8042 - val_loss: 0.4572\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 354ms/step - accuracy: 0.7422 - loss: 0.5090 - val_accuracy: 0.8042 - val_loss: 0.4339\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 346ms/step - accuracy: 0.7828 - loss: 0.4494 - val_accuracy: 0.7762 - val_loss: 0.4250\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 345ms/step - accuracy: 0.8114 - loss: 0.4145 - val_accuracy: 0.8112 - val_loss: 0.4021\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 351ms/step - accuracy: 0.8062 - loss: 0.3948 - val_accuracy: 0.8112 - val_loss: 0.4043\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 347ms/step - accuracy: 0.8267 - loss: 0.3844 - val_accuracy: 0.7832 - val_loss: 0.4001\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 346ms/step - accuracy: 0.7989 - loss: 0.3925 - val_accuracy: 0.8322 - val_loss: 0.3800\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 349ms/step - accuracy: 0.8491 - loss: 0.3478 - val_accuracy: 0.8182 - val_loss: 0.3690\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 347ms/step - accuracy: 0.8513 - loss: 0.3499 - val_accuracy: 0.7972 - val_loss: 0.3993\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 346ms/step - accuracy: 0.8302 - loss: 0.3324 - val_accuracy: 0.8462 - val_loss: 0.3512\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.8722 - loss: 0.2858\n",
            "\n",
            "Précision finale sur le TEST : 87.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installer Gradio\n",
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "def predict_helmet(img):\n",
        "    # Prétraitement de l'image\n",
        "    img = img.resize((224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Même normalisation que l'entraînement\n",
        "    img_array = np.expand_dims(img_array, axis=0) # Ajouter la dimension 'batch'\n",
        "\n",
        "    # Prédiction\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "\n",
        "    # Interprétation du résultat (Sigmoid : 0=Sans, 1=Avec)\n",
        "    # On inverse souvent selon l'ordre alphabétique des dossiers\n",
        "    # with_helmet = 0 ou 1 selon le train_generator.class_indices\n",
        "    # Vérifions l'ordre :\n",
        "    labels = {v: k for k, v in train_generator.class_indices.items()}\n",
        "\n",
        "    if prediction > 0.5:\n",
        "        res = labels[1].replace(\"_\", \" \").title()\n",
        "        conf = float(prediction)\n",
        "    else:\n",
        "        res = labels[0].replace(\"_\", \" \").title()\n",
        "        conf = float(1 - prediction)\n",
        "\n",
        "    return f\"Résultat : {res} (Confiance : {conf:.2%})\"\n",
        "\n",
        "# 2. Créer l'interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_helmet,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Système de Détection de Casque\",\n",
        "    description=\"Téléchargez une photo d'un conducteur pour vérifier s'il porte un casque.\"\n",
        ")\n",
        "\n",
        "# 3. Lancer l'interface\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "6muiMHi0iwoW",
        "outputId": "bda9819b-e062-40e3-d5b4-2b560ccdbed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://50e734054570e57206.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50e734054570e57206.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}